{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Before-your-start:\" data-toc-modified-id=\"Before-your-start:-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Before your start:</a></span></li><li><span><a href=\"#Challenge-1---Import-and-Describe-the-Dataset\" data-toc-modified-id=\"Challenge-1---Import-and-Describe-the-Dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Challenge 1 - Import and Describe the Dataset</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Explore-the-dataset-with-mathematical-and-visualization-techniques.-What-do-you-find?\" data-toc-modified-id=\"Explore-the-dataset-with-mathematical-and-visualization-techniques.-What-do-you-find?-2.0.0.1\"><span class=\"toc-item-num\">2.0.0.1&nbsp;&nbsp;</span>Explore the dataset with mathematical and visualization techniques. What do you find?</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-2---Data-Cleaning-and-Transformation\" data-toc-modified-id=\"Challenge-2---Data-Cleaning-and-Transformation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Challenge 2 - Data Cleaning and Transformation</a></span></li><li><span><a href=\"#Challenge-3---Data-Preprocessing\" data-toc-modified-id=\"Challenge-3---Data-Preprocessing-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Challenge 3 - Data Preprocessing</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#We-will-use-the-StandardScaler-from-sklearn.preprocessing-and-scale-our-data.-Read-more-about-StandardScaler-here.\" data-toc-modified-id=\"We-will-use-the-StandardScaler-from-sklearn.preprocessing-and-scale-our-data.-Read-more-about-StandardScaler-here.-4.0.0.1\"><span class=\"toc-item-num\">4.0.0.1&nbsp;&nbsp;</span>We will use the <code>StandardScaler</code> from <code>sklearn.preprocessing</code> and scale our data. Read more about <code>StandardScaler</code> <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler\" target=\"_blank\">here</a>.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-4---Data-Clustering-with-K-Means\" data-toc-modified-id=\"Challenge-4---Data-Clustering-with-K-Means-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Challenge 4 - Data Clustering with K-Means</a></span></li><li><span><a href=\"#Challenge-5---Data-Clustering-with-DBSCAN\" data-toc-modified-id=\"Challenge-5---Data-Clustering-with-DBSCAN-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Challenge 5 - Data Clustering with DBSCAN</a></span></li><li><span><a href=\"#Challenge-6---Compare-K-Means-with-DBSCAN\" data-toc-modified-id=\"Challenge-6---Compare-K-Means-with-DBSCAN-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Challenge 6 - Compare K-Means with DBSCAN</a></span></li><li><span><a href=\"#Bonus-Challenge-2---Changing-K-Means-Number-of-Clusters\" data-toc-modified-id=\"Bonus-Challenge-2---Changing-K-Means-Number-of-Clusters-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Bonus Challenge 2 - Changing K-Means Number of Clusters</a></span></li><li><span><a href=\"#Bonus-Challenge-3---Changing-DBSCAN-eps-and-min_samples\" data-toc-modified-id=\"Bonus-Challenge-3---Changing-DBSCAN-eps-and-min_samples-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Bonus Challenge 3 - Changing DBSCAN <code>eps</code> and <code>min_samples</code></a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before your start:\n",
    "- Read the README.md file\n",
    "- Comment as much as you can and use the resources in the README.md file\n",
    "- Happy learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your libraries:\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings                                              \n",
    "from sklearn.exceptions import DataConversionWarning          \n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1 - Import and Describe the Dataset\n",
    "\n",
    "In this lab, we will use a dataset containing information about customer preferences. We will look at how much each customer spends in a year on each subcategory in the grocery store and try to find similarities using clustering.\n",
    "\n",
    "The origin of the dataset is [here](https://archive.ics.uci.edu/ml/datasets/wholesale+customers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data: Wholesale customers data\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV into Pandas DataFrame\n",
    "file_path = '..\\data\\Wholesale customers data.csv'  \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display first few rows of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Display basic information about the DataFrame\n",
    "print(df.info())\n",
    "\n",
    "# Display summary statistics\n",
    "print(df.describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the dataset with mathematical and visualization techniques. What do you find?\n",
    "\n",
    "Checklist:\n",
    "\n",
    "* What does each column mean?\n",
    "* Any categorical data to convert?\n",
    "* Any missing data to remove?\n",
    "* Column collinearity - any high correlations?\n",
    "* Descriptive statistics - any outliers to remove?\n",
    "* Column-wise data distribution - is the distribution skewed?\n",
    "* Etc.\n",
    "\n",
    "Additional info: Over a century ago, an Italian economist named Vilfredo Pareto discovered that roughly 20% of the customers account for 80% of the typical retail sales. This is called the [Pareto principle](https://en.wikipedia.org/wiki/Pareto_principle). Check if this dataset displays this characteristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Calculate Column Colinearity (Correlation Matrix)\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# 2. Descriptive Statistics\n",
    "descriptive_stats = df.describe(include='all')\n",
    "\n",
    "# 3. Column-wise Data Distribution (Value Counts for Categorical, Histogram for Numeric)\n",
    "distribution = {}\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == 'object' or df[column].dtype.name == 'category':\n",
    "        distribution[column] = df[column].value_counts()\n",
    "    else:\n",
    "        distribution[column] = df[column].describe()\n",
    "\n",
    "\n",
    "# 1. Correlation Heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.matshow(df.corr(), fignum=1, cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.title('Correlation Heatmap', fontsize=14, pad=20)\n",
    "plt.xticks(ticks=range(df.shape[1]), labels=df.columns, rotation=45, ha='left')\n",
    "plt.yticks(ticks=range(df.shape[1]), labels=df.columns)\n",
    "plt.show()\n",
    "\n",
    "# 2. Histograms for Numerical Columns\n",
    "numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "for column in numeric_columns:\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.hist(df[column], bins=30, alpha=0.7, edgecolor='black')\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "# 3. Bar Plots for Categorical Columns\n",
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "for column in categorical_columns:\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    df[column].value_counts().plot(kind='bar', alpha=0.7, edgecolor='black')\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness = df.skew()\n",
    "\n",
    "# Convert the pandas series to a DataFrame\n",
    "skewness_df = skewness.reset_index()\n",
    "skewness_df.columns = ['Variable', 'Skewness']\n",
    "\n",
    "# Determine skewness type based on value\n",
    "def determine_skewness(value):\n",
    "    if value < -1:\n",
    "        return 'Highly negatively skewed'\n",
    "    elif -1 <= value < -0.5:\n",
    "        return 'Moderately negatively skewed'\n",
    "    elif -0.5 <= value <= 0.5:\n",
    "        return 'Approximately symmetric'\n",
    "    elif 0.5 < value <= 1:\n",
    "        return 'Moderately positively skewed'\n",
    "    else:\n",
    "        return 'Highly positively skewed'\n",
    "\n",
    "# Add Skewness Type column\n",
    "skewness_df['Skewness Type'] = skewness_df['Skewness'].apply(determine_skewness)\n",
    "\n",
    "print(\"\\n--- Skewness Analysis from Series ---\")\n",
    "print(skewness_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Z-score method\n",
    "z_scores = stats.zscore(df.select_dtypes(include=['int64', 'float64']))\n",
    "outliers = (abs(z_scores) > 3).any(axis=1)\n",
    "\n",
    "# Show outliers\n",
    "print(\"\\n--- Outliers Detected (Z-score > 3) ---\")\n",
    "print(df[outliers])\n",
    "\n",
    "# Remove outliers\n",
    "df_no_outliers = df[~outliers]\n",
    "print(\"\\n--- DataFrame After Removing Outliers ---\")\n",
    "print(df_no_outliers.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What does each column mean?\n",
    "FRESH: annual spending (m.u.) on fresh products (Continuous)\n",
    "MILK: annual spending (m.u.) on milk products (Continuous)\n",
    "GROCERY: annual spending (m.u.) on grocery products (Continuous)\n",
    "FROZEN: annual spending (m.u.) on frozen products (Continuous)\n",
    "DETERGENTS_PAPER: annual spending (m.u.) on detergents and paper products (Continuous)\n",
    "DELICATESSEN: annual spending (m.u.) on delicatessen products (Continuous)\n",
    "CHANNEL: customers' Channel - Horeca (Hotel/Restaurant/Café) or Retail channel (Nominal)\n",
    "REGION: customers' Region – Lisbon, Oporto or Other (Nominal)\n",
    "\n",
    "* Any categorical data to convert?\n",
    "\n",
    "CHANNEL and REGION are categoricals\n",
    "\n",
    "* Any missing data to remove?\n",
    "\n",
    "No missing data. \n",
    "\n",
    "* Column collinearity - any high correlations?\n",
    "\n",
    "Grocery & Detergents_Paper: Very strong correlation. \n",
    "Milk & Grocery: Strong positive correlation.\n",
    "\n",
    "* Descriptive statistics - any outliers to remove?\n",
    "\n",
    "--- Outliers Detected (Z-score > 3) ---\n",
    "     Channel  Region   Fresh   Milk  Grocery  Frozen  Detergents_Paper  \\\n",
    "23         2       3   26373  36423    22019    5154              4337   \n",
    "39         1       3   56159    555      902   10002               212   \n",
    "47         2       3   44466  54259    55571    7782             24171   \n",
    "56         2       3    4098  29892    26866    2616             17740   \n",
    "61         2       3   35942  38369    59598    3254             26701   \n",
    "65         2       3      85  20959    45828      36             24231   \n",
    "71         1       3   18291   1266    21042    5373              4173   \n",
    "...\n",
    "413        2435  \n",
    "* Column-wise data distribution - is the distribution skewed?\n",
    "\n",
    "--- Skewness Analysis from Series ---\n",
    "        Variable  Skewness                Skewness Type\n",
    "         Channel  0.760951 Moderately positively skewed\n",
    "          Region -1.283627     Highly negatively skewed\n",
    "           Fresh  2.561323     Highly positively skewed\n",
    "            Milk  4.053755     Highly positively skewed\n",
    "         Grocery  3.587429     Highly positively skewed\n",
    "          Frozen  5.907986     Highly positively skewed\n",
    "Detergents_Paper  3.631851     Highly positively skewed\n",
    "      Delicassen 11.151586     Highly positively skewed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2 - Data Cleaning and Transformation\n",
    "\n",
    "If your conclusion from the previous challenge is the data need cleaning/transformation, do it in the cells below. However, if your conclusion is the data need not be cleaned or transformed, feel free to skip this challenge. But if you do choose the latter, please provide rationale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Besides removing outliers, the data does not require any further preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your comment here**\n",
    "\n",
    "-  Besides removing outliers, the data does not require any further preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your import here:\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select the numeric columns\n",
    "numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the numeric columns\n",
    "customers_scale = df.copy()\n",
    "customers_scale[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
    "\n",
    "# Display the first few rows of the scaled dataset\n",
    "print(\"\\n--- Scaled DataFrame (using StandardScaler) ---\")\n",
    "print(customers_scale.head())\n",
    "\n",
    "# Show summary statistics of scaled data\n",
    "print(\"\\n--- Summary Statistics of Scaled Data ---\")\n",
    "print(customers_scale.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 4 - Data Clustering with K-Means\n",
    "\n",
    "Now let's cluster the data with K-Means first. Initiate the K-Means model, then fit your scaled data. In the data returned from the `.fit` method, there is an attribute called `labels_` which is the cluster number assigned to each data record. What you can do is to assign these labels back to `customers` in a new column called `customers['labels']`. Then you'll see the cluster results of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Determine Optimal Number of Clusters Using Elbow Method\n",
    "inertia = []\n",
    "k_range = range(1, 11)\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(customers_scale)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(k_range, inertia, marker='o')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Fit K-Means with Optimal Number of Clusters (Assume k=3)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(customers_scale)\n",
    "\n",
    "# Add cluster labels to dataframe\n",
    "df['Cluster'] = clusters\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(\n",
    "    customers_scale.iloc[:, 0],  # First principal dimension\n",
    "    customers_scale.iloc[:, 1],  # Second principal dimension\n",
    "    c=df['Cluster'].values,      # Use cluster labels for coloring\n",
    "    cmap='viridis',\n",
    "    edgecolors='k'\n",
    ")\n",
    "plt.title('K-Means Clustering (First Two Dimensions)')\n",
    "plt.xlabel('Fresh (scaled)')\n",
    "plt.ylabel('Milk (scaled)')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# trying out PCA to visualize the clusters\n",
    "\n",
    "# Fit PCA with 2 principal components\n",
    "pca_2d = PCA(n_components=2)\n",
    "pca_2d_data = pca_2d.fit_transform(customers_scale) \n",
    "\n",
    "# 2. Plot the 2D PCA result\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(\n",
    "    pca_2d_data[:, 0],\n",
    "    pca_2d_data[:, 1],\n",
    "    c=df['Cluster'].values,  # color by cluster labels\n",
    "    cmap='viridis',\n",
    "    edgecolors='k'\n",
    ")\n",
    "plt.title('K-Means Clusters in 2D (PCA)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# (Optional) Print explained variance ratio\n",
    "print(\"Explained Variance Ratio (2D):\", pca_2d.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D  \n",
    "\n",
    "\n",
    "# trying pca with 3 principal components, to visualize the clusters in 3D\n",
    "\n",
    "# Fit PCA with 3 principal components\n",
    "pca_3d = PCA(n_components=3)\n",
    "pca_3d_data = pca_3d.fit_transform(customers_scale)\n",
    "\n",
    "# Create a 3D plot\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "sc = ax.scatter(\n",
    "    pca_3d_data[:, 0],\n",
    "    pca_3d_data[:, 1],\n",
    "    pca_3d_data[:, 2],\n",
    "    c=df['Cluster'].values,\n",
    "    cmap='viridis',\n",
    "    edgecolors='k'\n",
    ")\n",
    "ax.set_title('K-Means Clusters in 3D (PCA)')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "plt.show()\n",
    "\n",
    "# (Optional) Print explained variance ratio\n",
    "print(\"Explained Variance Ratio (3D):\", pca_3d.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking to the elbow we can choose 2 like the correct number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_2 = KMeans(n_clusters=2).fit(customers_scale)\n",
    "\n",
    "labels = kmeans_2.predict(customers_scale)\n",
    "\n",
    "clusters = kmeans_2.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_customers = df.copy()\n",
    "\n",
    "clean_customers['Label'] = clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the values in `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "label_counts = Counter(labels)\n",
    "print(label_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3 - Data Preprocessing\n",
    "\n",
    "One problem with the dataset is the value ranges are remarkably different across various categories (e.g. `Fresh` and `Grocery` compared to `Detergents_Paper` and `Delicassen`). If you made this observation in the first challenge, you've done a great job! This means you not only completed the bonus questions in the previous Supervised Learning lab but also researched deep into [*feature scaling*](https://en.wikipedia.org/wiki/Feature_scaling). Keep on the good work!\n",
    "\n",
    "Diverse value ranges in different features could cause issues in our clustering. The way to reduce the problem is through feature scaling. We'll use this technique again with this dataset.\n",
    "\n",
    "#### We will use the `StandardScaler` from `sklearn.preprocessing` and scale our data. Read more about `StandardScaler` [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler).\n",
    "\n",
    "*After scaling your data, assign the transformed data to a new variable `customers_scale`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 5 - Data Clustering with DBSCAN\n",
    "\n",
    "Now let's cluster the data using DBSCAN. Use `DBSCAN(eps=0.5)` to initiate the model, then fit your scaled data. In the data returned from the `.fit` method, assign the `labels_` back to `customers['labels_DBSCAN']`. Now your original data have two labels, one from K-Means and the other from DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN \n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize DBSCAN with eps=0.5\n",
    "dbscan = DBSCAN(eps=0.5)\n",
    "\n",
    "# Fit DBSCAN on the scaled data\n",
    "dbscan.fit(customers_scale)\n",
    "\n",
    "# Retrieve cluster labels and store them in labels_DBSCAN (-1 indicates noise)\n",
    "labels_DBSCAN = dbscan.labels_\n",
    "\n",
    "# Count how many points fall into each cluster (including noise)\n",
    "label_counts = Counter(labels_DBSCAN)\n",
    "print(\"DBSCAN cluster counts:\", label_counts)\n",
    "\n",
    "# Visualize the clusters using the first two dimensions of the scaled data\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(\n",
    "    customers_scale.iloc[:, 0],  # First feature using iloc\n",
    "    customers_scale.iloc[:, 1],  # Second feature using iloc\n",
    "    c=labels_DBSCAN,                # Color by cluster label\n",
    "    cmap='viridis',\n",
    "    edgecolors='k'\n",
    ")\n",
    "plt.title(\"DBSCAN Clustering (eps=0.5)\")\n",
    "plt.xlabel(\"Feature 1 (scaled)\")\n",
    "plt.ylabel(\"Feature 2 (scaled)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the values in `labels_DBSCAN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "label_counts = Counter(labels_DBSCAN)\n",
    "print(\"DBSCAN cluster counts:\", label_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 6 - Compare K-Means with DBSCAN\n",
    "\n",
    "Now we want to visually compare how K-Means and DBSCAN have clustered our data. We will create scatter plots for several columns. For each of the following column pairs, plot a scatter plot using `labels` and another using `labels_DBSCAN`. Put them side by side to compare. Which clustering algorithm makes better sense?\n",
    "\n",
    "Columns to visualize:\n",
    "\n",
    "* `Detergents_Paper` as X and `Milk` as y\n",
    "* `Grocery` as X and `Fresh` as y\n",
    "* `Frozen` as X and `Delicassen` as y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the column pairs to compare\n",
    "column_pairs = [\n",
    "    ('Detergents_Paper', 'Milk'),\n",
    "    ('Grocery', 'Fresh'),\n",
    "    ('Frozen', 'Delicassen')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize `Detergents_Paper` as X and `Milk` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure with 1 row and 2 columns\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('Clustering Comparison: Detergents_Paper vs. Milk', fontsize=16)\n",
    "\n",
    "# Left subplot: K-Means Clustering\n",
    "axes[0].scatter(df['Detergents_Paper'], df['Milk'], c=labels, cmap='viridis', edgecolors='k')\n",
    "axes[0].set_title('K-Means Clustering')\n",
    "axes[0].set_xlabel('Detergents_Paper')\n",
    "axes[0].set_ylabel('Milk')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Right subplot: DBSCAN Clustering\n",
    "axes[1].scatter(df['Detergents_Paper'], df['Milk'], c=labels_DBSCAN, cmap='viridis', edgecolors='k')\n",
    "axes[1].set_title('DBSCAN Clustering')\n",
    "axes[1].set_xlabel('Detergents_Paper')\n",
    "axes[1].set_ylabel('Milk')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize `Grocery` as X and `Fresh` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure with 1 row and 2 columns\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('Clustering Comparison: Grocery vs. Fresh', fontsize=16)\n",
    "\n",
    "# Left subplot: K-Means Clustering\n",
    "axes[0].scatter(df['Grocery'], df['Fresh'], c=labels, cmap='viridis', edgecolors='k')\n",
    "axes[0].set_title('K-Means Clustering')\n",
    "axes[0].set_xlabel('Grocery')\n",
    "axes[0].set_ylabel('Fresh')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Right subplot: DBSCAN Clustering\n",
    "axes[1].scatter(df['Grocery'], df['Fresh'], c=labels_DBSCAN, cmap='viridis', edgecolors='k')\n",
    "axes[1].set_title('DBSCAN Clustering')\n",
    "axes[1].set_xlabel('Grocery')\n",
    "axes[1].set_ylabel('Fresh')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize `Frozen` as X and `Delicassen` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure with 1 row and 2 columns\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('Clustering Comparison: Frozen vs. Delicassen', fontsize=16)\n",
    "\n",
    "# Left subplot: K-Means Clustering\n",
    "axes[0].scatter(df['Frozen'], df['Delicassen'], c=labels, cmap='viridis', edgecolors='k')\n",
    "axes[0].set_title('K-Means Clustering')\n",
    "axes[0].set_xlabel('Frozen')\n",
    "axes[0].set_ylabel('Delicassen')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Right subplot: DBSCAN Clustering\n",
    "axes[1].scatter(df['Frozen'], df['Delicassen'], c=labels_DBSCAN, cmap='viridis', edgecolors='k')\n",
    "axes[1].set_title('DBSCAN Clustering')\n",
    "axes[1].set_xlabel('Frozen')\n",
    "axes[1].set_ylabel('Delicassen')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a groupby to see how the mean differs between the groups. Group `customers` by `labels` and `labels_DBSCAN` respectively and compute the means for all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by K-Means clusters and compute mean values\n",
    "kmeans_group_means = clean_customers.groupby(labels).mean()\n",
    "print(\"Mean values by K-Means clusters:\")\n",
    "print(kmeans_group_means)\n",
    "\n",
    "# Group by DBSCAN clusters and compute mean values\n",
    "dbscan_group_means = clean_customers.groupby(labels_DBSCAN).mean()\n",
    "print(\"\\nMean values by DBSCAN clusters:\")\n",
    "print(dbscan_group_means)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which algorithm appears to perform better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your observations here**\n",
    "\n",
    "K-Means appears to perform better. It creates two clear groups with distinct average behaviors.\n",
    "The differences in means between clusters are easier to interpret and can be directly tied to customer purchasing behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge 2 - Changing K-Means Number of Clusters\n",
    "\n",
    "As we mentioned earlier, we don't need to worry about the number of clusters with DBSCAN because it automatically decides that based on the parameters we send to it. But with K-Means, we have to supply the `n_clusters` param (if you don't supply `n_clusters`, the algorithm will use `8` by default). You need to know that the optimal number of clusters differs case by case based on the dataset. K-Means can perform badly if the wrong number of clusters is used.\n",
    "\n",
    "In advanced machine learning, data scientists try different numbers of clusters and evaluate the results with statistical measures (read [here](https://en.wikipedia.org/wiki/Cluster_analysis#External_evaluation)). We are not using statistical measures today but we'll use our eyes instead. In the cells below, experiment with different number of clusters and visualize with scatter plots. What number of clusters seems to work best for K-Means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# different k values to experiment with\n",
    "k_values = [2, 3, 4, 5]\n",
    "\n",
    "# Create subplots for each k value\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, k in enumerate(k_values):\n",
    "    # Initialize and fit K-Means with k clusters\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans_labels = kmeans.fit_predict(customers_scale)\n",
    "    \n",
    "    # Create a scatter plot using Detergents_Paper as X and Milk as Y\n",
    "    axes[i].scatter(df['Detergents_Paper'], df['Milk'], c=kmeans_labels, cmap='viridis', edgecolors='k')\n",
    "    axes[i].set_title(f'K-Means Clustering with k = {k}')\n",
    "    axes[i].set_xlabel('Detergents_Paper')\n",
    "    axes[i].set_ylabel('Milk')\n",
    "    axes[i].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your comment here**\n",
    "\n",
    "From a purely visual standpoint, k=3 seems to strike a good balance between capturing the major distinctions in the data and avoiding overfragmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge 3 - Changing DBSCAN `eps` and `min_samples`\n",
    "\n",
    "Experiment changing the `eps` and `min_samples` params for DBSCAN. See how the results differ with scatter plot visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from collections import Counter\n",
    "\n",
    "# Example ranges for eps and min_samples\n",
    "eps_values = [0.3, 0.5, 0.7]\n",
    "min_samples_values = [5, 10]\n",
    "\n",
    "# Create a grid of subplots: rows = len(min_samples_values), cols = len(eps_values)\n",
    "fig, axes = plt.subplots(nrows=len(min_samples_values), ncols=len(eps_values), figsize=(15, 8))\n",
    "\n",
    "for row, ms in enumerate(min_samples_values):\n",
    "    for col, e in enumerate(eps_values):\n",
    "        # Initialize and fit DBSCAN with the current eps and min_samples\n",
    "        dbscan = DBSCAN(eps=e, min_samples=ms)\n",
    "        db_labels = dbscan.fit_predict(customers_scale)\n",
    "        \n",
    "        # Plot the results on the corresponding subplot\n",
    "        ax = axes[row, col]\n",
    "        scatter = ax.scatter(\n",
    "            df['Detergents_Paper'],\n",
    "            df['Milk'],\n",
    "            c=db_labels,\n",
    "            cmap='viridis',\n",
    "            edgecolors='k'\n",
    "        )\n",
    "        ax.set_title(f\"DBSCAN (eps={e}, min_samples={ms})\")\n",
    "        ax.set_xlabel('Detergents_Paper')\n",
    "        ax.set_ylabel('Milk')\n",
    "        ax.grid(True)\n",
    "        \n",
    "        # Count the clusters (including noise, labeled -1)\n",
    "        cluster_counts = Counter(db_labels)\n",
    "        print(f\"eps={e}, min_samples={ms}, Cluster Counts: {cluster_counts}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your comment here**\n",
    "\n",
    "Increasing eps generally reduces the number of noise points and merges clusters.\n",
    "Increasing min_samples increases the density requirement, leading to fewer, larger clusters (or more noise)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
